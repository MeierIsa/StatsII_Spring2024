asds <-2023
# Now try calling x.
x <- Hello World
# Now try calling x.
x <- Hello world
# Now try calling x.
x  "Hello world"
View (data_nile)
View(data_nile)
plot(Nile)
plot(Nile)
# Now try calling x.
x <- "Hello world"
# R calculates stuff, e.g., add 2 and 2
2 + 2
# Create 7 random numbers between 0 and 1
runif(7)
z = 2 + 2
data_nile
# "c" stands for concatenate so "y" here stores the values 1, 2 and 4.
y <- c(1, 2, 4)
#Parts of individual objects can be accessed via square brackets
y[3]
# We can access multiple parts of objects with a colon
y[2:3]
data_chick[1,4]
data_chick[1]
y<- runif(10)
# Suppose now we want the sum of y
sum(y)
sum(y_norm)
sum(chickwts)
sum(ChickWeight)
# Store the output of a function in another object
z <- sum(y)
# Can you use the sqrt() function to compute the square root of 962?
sqrt(962)
# Can you now assign to a new object?
a <- sqrt(962)
a+5
sum(ChickWeight)
chickwts[1,]
rm(list = ls())
x <- "Hello"
class(5)
# We can use square brackets [] after an object's name to
# access a particular element by its *index*. Indexes in R
# begin at 1. x[1] is therefore "Hello". We can use []
# together with the *assignment operator* <- to add an
# element to x, here the word "world", which will be stored
# in the second index position.
x[1]
x[2] <- "world"
# Extract the element named gamma from the vector below.
vec2 <- c(alpha = 1, beta = 2, gamma = 3)
vec[gama]
vec2[gama]
vec2[gamma]
vec2["gamma"]
x <- rnorm(n = 50)
length(x)
mean(x)
sd(x)
# Use rnrom() to generate 100 random normal values
# with a mean of 100 and a standard deviation of 15.
x<-rnorm(n=100)
mean(x)
rnorm(100, mean=100, sd=15)
data<-rnorm(100, mean=100, sd=15)
lenght(data)
hist(data)
set.seed(2)
lenght(data)
length(data)
y <- x
y <- 2*y
xylm <- lm(y~x)
xylm
attributes(xylm)
xylm$coefficients
summary(xylm)
plot(x,y)
xylm$residuals
plot(xylm$residuals)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```{r diamonds-head}
head(diamonds)
knitr::kable(head(diamonds), caption = "a knitr kable")
```{r fancy-diamonds-head}
knitr::kable(head(diamonds), caption = "a knitr kable")
```{r fancy-diamonds-head}
knitr::kable(head(diamonds), caption = "a knitr kable")
knitr::kable(head(diamonds), caption = "a knitr kable")
knitr::kable(head(diamonds), caption = "a knitr kable")
knitr::kable(head(diamonds), caption = "a knitr kable")
knitr::kable(head(diamonds), caption = "a knitr kable")
knitr::kable(head(diamonds), caption = "a knitr kable")
```{r fancy-diamonds-head}
knitr::kable(head(diamonds), caption = "a knitr kable")
knitr::kable(head(diamonds), caption = "a knitr kable")
diamonds %>%
filter(cut %in% c("Ideal", "Premium", "Very Good")) %>%
group_by(cut) %>%
ggplot(aes(cut, price)) +
geom_boxplot() +
labs(title = "Average Diamond Price ($) by Quality of Cut")
knitr::kable(head(diamonds), caption = "a knitr kable")
knitr::kable(head(diamonds), caption = "a knitr kable")
knitr::kable(head(diamonds), caption = "a knitr kable")
diamonds %>%
filter(cut %in% c("Ideal", "Premium", "Very Good")) %>%
group_by(cut) %>%
ggplot(aes(cut, price)) +
geom_boxplot() +
labs(title = "Average Diamond Price ($) by Quality of Cut")
install.packages("knitr")
knitr::kable(head(diamonds), caption = "a knitr kable")
install.packages("knitr")
install.packages("knitr")
install.packages("knitr")
install.packages("knitr")
install.packages("knitr")
install.packages("knitr")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
install.packages("knitr")
knitr::kable(head(diamonds), caption = "a knitr kable")
install.packages("knitr")
library(knitr)
install.packages("knitr")
knitr::kable(head(diamonds), caption = "a knitr kable")
diamonds %>%
filter(cut %in% c("Ideal", "Premium", "Very Good")) %>%
group_by(cut) %>%
ggplot(aes(carat, price, color = cut)) +
geom_smooth() +
labs(title = "Diamond Price by Cut and Weight", subtitle = "The Bigger the Diamond, the Lower the Quality")
## Load packages
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("tidyverse",
"guardianapi",
"quanteda",
"lubridate",
"quanteda.textmodels",
"quanteda.textstats",
"caret", # For train/test split
"MLmetrics", # For ML
"doParallel"), # For parallel processing
pkgTest)
## Load packages
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("tidyverse",
"guardianapi",
"quanteda",
"lubridate",
"quanteda.textmodels",
"quanteda.textstats",
"caret", # For train/test split
"MLmetrics", # For ML
"doParallel"), # For parallel processing
pkgTest)
dat <- readRDS("data/df2023") # try this with different data.frames
#############################
# Tutorial 4: Supervised ML #
#############################
setwd("/Users/isabellameier/Documents/GitHub/QTA_Spring23/tutorials/tutorial04")
## Load packages
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("tidyverse",
"guardianapi",
"quanteda",
"lubridate",
"quanteda.textmodels",
"quanteda.textstats",
"caret", # For train/test split
"MLmetrics", # For ML
"doParallel"), # For parallel processing
pkgTest)
dat <- readRDS("data/df2023") # try this with different data.frames
dat <- dat[dat$section_name %in% c("World news", "Opinion")
& dat$type == "article",]
dat <- dat %>%
select(headline,
byline,
date = web_publication_date, # Rename date variable
section_name,
standfirst,
body_text
) %>%
mutate(date = as_datetime(date)) # parse date
dat <- dat[-which(duplicated(dat$headline)),]
dat$section_name <- ifelse(dat$section_name == "World news", "World", dat$section_name)
## 2. Preparation
dat$body_text <- str_replace(dat$body_text, "\u2022.+$", "")
corp <- corpus(dat,
docid_field = "headline",
text_field = "body_text")
# Clean and make tokens
source("code/pre_processing.R")
prepped_toks <- prep_toks(corp) # basic token cleaning
collocations <- get_coll(prepped_toks) # get collocations
toks <- tokens_compound(prepped_toks, pattern = collocations[collocations$z > 10,]) # replace collocations
toks <- tokens_remove(tokens(toks), "") # let's also remove the whitespace placeholders
toks <- tokens(toks,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE) # remove other uninformative text
# Create dfm and weight using tf/idf
dfm <- dfm(toks) # create DFM
View(dfm)
View(collocations)
View(dat)
dfm <- dfm_trim(dfm, min_docfreq = 10) # trim DFM
dfm <- dfm_tfidf(dfm) # weight DFM
# Convert dfm to dataframe for ML
tmpdata <- convert(dfm, to = "data.frame", docvars = NULL)
tmpdata <- tmpdata[, -1] # drop document id variable (first variable)
section_labels <- dfm@docvars$section_name # get section labels - note, the @ operator is specific to S4 class object
tmpdata <- as.data.frame(cbind(section_labels, tmpdata)) # labelled data frame
View(tmpdata)
## 3. ML Preparation
# You need to a) Create a 5% validation split
set.seed(2023) # set seed for replicability
tmpdata <- tmpdata[sample(nrow(tmpdata)), ] # randomly order labelled dataset
split <- round(nrow(tmpdata) * 0.05) # determine cutoff point of 5% of documents
vdata <- tmpdata[1:split, ] # validation set
ldata <- tmpdata[(split + 1):nrow(tmpdata), ] # labelled dataset minus validation set
View(vdata)
#             b) Create an 80/20 test/train split
train_row_nums <- createDataPartition(ldata$section_labels,
p=0.8,
list=FALSE) # set human_labels as the Y variable in caret
Train <- ldata[train_row_nums, ] # training set
Test <- ldata[-train_row_nums, ] # testing set
View(Train)
#             c) Create five-fold cross validation with 3 repeats object - to supply to train()
train_control <- trainControl(
method = "repeatedcv",
number = 5,
repeats = 3,
classProbs= TRUE,
summaryFunction = multiClassSummary,
selectionFunction = "best", # select the model with the best performance metric
verboseIter = TRUE
)
View(train_control)
## 4. Naive Bayes classification
# You need to a) Check the parameters for Naive Bayes algorithm
modelLookup(model = "naive_bayes")
#             b) Create a matrix of combinations of parameters to supply to tuneGrid arg of train()
tuneGrid <- expand.grid(laplace = c(0,0.5,1.0),
usekernel = c(TRUE, FALSE),
adjust=c(0.75, 1, 1.25, 1.5))
tuneGrid
View(tuneGrid)
View(tuneGrid)
#             c) Set up parallel processing
cl <- makePSOCKcluster(6) # create number of copies of R to run in parallel and communicate over sockets
# Note that the number of clusters depends on how many cores your machine has.
registerDoParallel(cl) # register parallel backed with foreach package
#             d) Train the model
nb_train <- train(section_labels ~ .,
data = Train,
method = "naive_bayes",
metric = "F1",
trControl = train_control,
tuneGrid = tuneGrid,
allowParallel= TRUE
)
#             b) Create a matrix of combinations of parameters to supply to tuneGrid arg of train()
tuneGrid <- expand.grid(laplace = c(0,0.5,1.0),
usekernel = c(TRUE, FALSE),
adjust=c(0.75, 1, 1.25, 1.5))
tuneGrid
#             c) Set up parallel processing
cl <- makePSOCKcluster(6) # create number of copies of R to run in parallel and communicate over sockets
# Note that the number of clusters depends on how many cores your machine has.
registerDoParallel(cl) # register parallel backed with foreach package
#             d) Train the model
nb_train <- train(section_labels ~ .,
data = Train,
method = "naive_bayes",
metric = "F1",
trControl = train_control,
tuneGrid = tuneGrid,
allowParallel= TRUE
)
#             e) Save the model!
saveRDS(nb_train, "data/nb_train")
View(nb_train)
#             g) Stop the cluster
stopCluster(cl) # stop parallel process once job is done
#             h) Evaluate performance
print(nb_train) # print cross-validation results
pred <- predict(nb_train, newdata = Test) # generate prediction on Test set using training set model
head(pred) # first few predictions
confusionMatrix(reference = as.factor(Test$section_labels), data = pred, mode='everything') # generate confusion matrix
#             i) Finalise the model
nb_final <- train(section_labels ~ .,
data = ldata,
method = "naive_bayes",
trControl = trainControl(method = "none"),
tuneGrid = data.frame(nb_train$bestTune))
#             j) Save the model!
saveRDS(nb_final, "data/nb_final")
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
library(mgcv)
lapply(c(),  pkgTest)
install.packages("gam")
library(gam)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
setwd("/Users/isabellameier/Documents/GitHub/StatsII_Spring2024/problemSets/PS02answers")
# load data
load(url("https://github.com/ASDS-TCD/StatsII_Spring2024/blob/main/datasets/climateSupport.RData?raw=true"))
head(climateSupport)
# Converting the "choice" column into 1s and 0s
climateSupport$choice <- as.numeric(climateSupport$choice == "Supported")
head(climateSupport)
#
sum(is.na(climateSupport$sanctions))
sum(is.infinite(climateSupport$sanctions))
climateSupport$countries_num <- as.numeric(sapply(strsplit(as.character(climateSupport$countries), "of"), "[", 1))
head(climateSupport$countries_num)
#
sanctions_mapping <- c("none" = 0, "5%" = 5, "15%" = 15, "20%" = 20)
climateSupport$sanctions_num <- sanctions_mapping[climateSupport$sanctions]
head(climateSupport$sanctions_num)
#
formula <- choice ~ ns(countries_num) + ns(sanctions_num)
model_g <- glm(formula, data = climateSupport, family = binomial(link = "logit"))
summary(model_g)
null_model <- glm(choice ~ 1, data = climateSupport, family = binomial(link = "logit"))
summary(null_model)
#
anova_res <- anova(null_model, model_g, test = "Chisq")
print(anova_res)
scenario1 <- climateSupport(countries_num = 160, sanctions_num = 5)
scenario1 <- data.frame(countries_num = 160, sanctions_num = 5)
logodds1 <- predict(model_g, newdata = scenario1, type = "link")
odds1 <- exp(logodds1)
print(odds1)
# scenario 2
scenario2 <- data.frame(countries_num = 160, sanctions_num = 15)
logodds2 <- predict(model_g, newdata = scenario2, type = "link")
odds2 <- exp(logodds2)
print(odds2)
coef_summary <- summary(model_g)$coefficients
coef_countries <- coef_summary["ns(countries_num)", "Estimate"]
coef_sanctions <- coef_summary["ns(sanctions_num)", "Estimate"]
oddschange1 <- exp(coef_countries * (scenario1$countries_num - scenario2$countries_num) +
coef_sanctions * (scenario1$sanctions_num - scenario2$sanctions_num))
print(oddschange1)
oddschange2 <- exp(coef_countries * (scenario2$countries_num - scenario1$countries_num) +
coef_sanctions * (scenario2$sanctions_num - scenario1$sanctions_num))
print(oddschange2)
# question 2 b
# defining scenario 3
scenario3 <- data.frame(countries_num = 80, sanctions_num = 0)
# redicting the probability of support
prob_support <- predict(model_g, newdata = scenario3, type = "response")
print(prob_support)
# question 2 c
modelg_int <- glm(choice ~ ns(countries_num) * ns(sanctions_num),
data = climateSupport, family = binomial(link = "logit"))
#
anova_intest <- anova(modelg_int, model_g, test = "Chisq")
print(anova_intest)
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
library(mgcv)
lapply(c(),  pkgTest)
install.packages("gam")
install.packages("gam")
library(gam)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
setwd("/Users/isabellameier/Documents/GitHub/StatsII_Spring2024/problemSets/PS02/PS02answers")
# load data
load(url("https://github.com/ASDS-TCD/StatsII_Spring2024/blob/main/datasets/climateSupport.RData?raw=true"))
head(climateSupport)
# Converting the "choice" column into 1s and 0s
climateSupport$choice <- as.numeric(climateSupport$choice == "Supported")
head(climateSupport)
#
sum(is.na(climateSupport$sanctions))
sum(is.infinite(climateSupport$sanctions))
climateSupport$countries_num <- as.numeric(sapply(strsplit(as.character(climateSupport$countries), "of"), "[", 1))
head(climateSupport$countries_num)
#
sanctions_mapping <- c("none" = 0, "5%" = 5, "15%" = 15, "20%" = 20)
climateSupport$sanctions_num <- sanctions_mapping[climateSupport$sanctions]
head(climateSupport$sanctions_num)
#
formula <- choice ~ ns(countries_num) + ns(sanctions_num)
model_g <- glm(formula, data = climateSupport, family = binomial(link = "logit"))
summary(model_g)
# fitting the model with no explanatory variables to get the null model
null_model <- glm(choice ~ 1, data = climateSupport, family = binomial(link = "logit"))
summary(null_model)
#
anova_res <- anova(null_model, model_g, test = "Chisq")
print(anova_res)
# QUESTION 2
# Defining scenario 1
scenario1 <- data.frame(countries_num = 160, sanctions_num = 5)
# Predicting the log odds of the scenario 1
logodds1 <- predict(model_g, newdata = scenario1, type = "link")
# Exponentiation the log odds to get odds ratio
odds1 <- exp(logodds1)
print(odds1)
# scenario 2
scenario2 <- data.frame(countries_num = 160, sanctions_num = 15)
logodds2 <- predict(model_g, newdata = scenario2, type = "link")
odds2 <- exp(logodds2)
print(odds2)
coef_summary <- summary(model_g)$coefficients
coef_countries <- coef_summary["ns(countries_num)", "Estimate"]
coef_sanctions <- coef_summary["ns(sanctions_num)", "Estimate"]
oddschange1 <- exp(coef_countries * (scenario1$countries_num - scenario2$countries_num) +
coef_sanctions * (scenario1$sanctions_num - scenario2$sanctions_num))
print(oddschange1)
oddschange2 <- exp(coef_countries * (scenario2$countries_num - scenario1$countries_num) +
coef_sanctions * (scenario2$sanctions_num - scenario1$sanctions_num))
print(oddschange2)
# question 2 b
# defining scenario 3
scenario3 <- data.frame(countries_num = 80, sanctions_num = 0)
# redicting the probability of support
prob_support <- predict(model_g, newdata = scenario3, type = "response")
print(prob_support)
# question 2 c
modelg_int <- glm(choice ~ ns(countries_num) * ns(sanctions_num),
data = climateSupport, family = binomial(link = "logit"))
#
anova_intest <- anova(modelg_int, model_g, test = "Chisq")
print(anova_intest)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(quanteda)
library(word2vec)
library(uwot)
text_csv <- readtext("breitbart.csv",
text_field = "text")
library(quanteda)
library(readtext)
library(word2vec)
text_csv <- readtext("breitbart.csv",
text_field = "text")
